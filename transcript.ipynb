{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDjXbwezzIWu",
        "outputId": "a2b70ea8-74bb-4aeb-8448-4c508291cb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition\n",
        "\n",
        "# Once the library is installed, you can create a script to convert the audio file to text. Here is a simple example: \n",
        "\n",
        "import speech_recognition as sr \n",
        "\n",
        "# Create an instance of the Recognizer \n",
        "r = sr.Recognizer() \n",
        "\n",
        "# Read the audio file \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZRVfKVswIA6",
        "outputId": "7543e4aa-14c9-400d-a801-f35789ff9a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.9.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 377 kB/s \n",
            "\u001b[?25hCollecting requests>=2.26.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.1.1)\n",
            "Installing collected packages: requests, SpeechRecognition\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed SpeechRecognition-3.9.0 requests-2.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "v2H8jupjwcOR",
        "outputId": "7392f80d-1778-4246-ae7e-f8c3872702eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10d7e795-2bfe-4b1c-8a26-fd269c27a7f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10d7e795-2bfe-4b1c-8a26-fd269c27a7f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving by using spech recognizer it is showing empty array - Google Search - Brave 2022-12-16 22-22-23.mp4 to by using spech recognizer it is showing empty array - Google Search - Brave 2022-12-16 22-22-23.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiHD7Pmg3Dvu",
        "outputId": "e103c1a5-76f0-42eb-e713-fa56bda6789b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "from os import path\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# convert mp3 file to wav                                                       \n",
        "# sound = AudioSegment.from_file(\"by using spech recognizer it is showing empty array - Google Search - Brave 2022-12-16 22-22-23.mp4\",\"mp4\")\n",
        "# sound.export(\"transcript.wav\", format=\"wav\")\n",
        "\n",
        "\n",
        "# transcribe audio file                                                         \n",
        "AUDIO_FILE = \"/content/drive/MyDrive/DAIC-WOZ/depression-detection/data/raw/audio/300_AUDIO.wav\"\n",
        "\n",
        "# use the audio file as the audio source                                        \n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(AUDIO_FILE) as source:\n",
        "        audio = r.record(source)  # read the entire audio file                  \n",
        "\n",
        "        print(\"Transcription: \")\n",
        "        print(r.recognize_google(audio, language = 'en', show_all = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7isSr9s0f5T",
        "outputId": "ea55678a-bec3-4ec5-867c-c6727985820b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription: \n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0rZxUVo_2gPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_sqlalchemy"
      ],
      "metadata": {
        "id": "Sk4fjsrqx92O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import configparser\n",
        "import re\n",
        "import ftfy\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from flask import Flask, render_template, request\n",
        "# from flask_sqlalchemy import SQLAlchemy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "ngI7lis8qi9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch"
      ],
      "metadata": {
        "id": "zLN6adChypry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "model_save_name = 'glove_model'\n",
        "path = F\"/content/drive/My Drive/capstone/{model_save_name}\"\n",
        "model = keras.models.load_model(path)"
      ],
      "metadata": {
        "id": "YDxnFzSl0MVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manuscipt = pd.read_csv('/content/drive/MyDrive/DAIC-WOZ/depression-detection/data/raw/transcripts/300_TRANSCRIPT.csv')"
      ],
      "metadata": {
        "id": "57vslaAV2CQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cList = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}"
      ],
      "metadata": {
        "id": "-pXrY95n3sW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)]\n",
        "    return c_re.sub(replace, text)\n",
        "\n",
        "pat1 = r'@[A-Za-z0-9_]+'\n",
        "pat2 = r'https?://[^ ]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "www_pat = r'www.[^ ]+'\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
      ],
      "metadata": {
        "id": "gf8xKyHa3-y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweets(tweets):\n",
        "    cleaned_tweets = []\n",
        "    for tweet in tweets:\n",
        "        tweet = str(tweet)\n",
        "        # if url links, then don't append to avoid news articles, etc.\n",
        "        # Check tweet length, save those > 6 (length of word \"lonely\")\n",
        "        if re.match(\"(\\w+:\\/\\/\\S+)\", tweet) == None and len(tweet) > 10:\n",
        "            tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "            tweet = re.sub(r'www.+', \"\", tweet)\n",
        "            #remove hashtags, @mention, emoji and image URLs\n",
        "            tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", tweet).split())\n",
        "            # Remove HTML special entities (e.g. &amp;)\n",
        "            tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
        "            #Convert @username to AT_USER\n",
        "            tweet = re.sub('@[^\\s]+','',tweet)\n",
        "            # Remove tickers\n",
        "            tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "            # To lowercase\n",
        "            tweet = tweet.lower()\n",
        "            # Remove hyperlinks\n",
        "            tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
        "            # Remove hashtags\n",
        "            #tweet = re.sub(r'#\\w*', '', tweet)\n",
        "            # Remove Punctuation and split 's, 't, 've with a space for filter\n",
        "            #tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)\n",
        "            # Remove words with 2 or fewer letters\n",
        "            tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
        "            # Remove whitespace (including new line characters)\n",
        "            tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
        "            # Remove single space remaining at the front of the tweet.\n",
        "            tweet = tweet.lstrip(' ') \n",
        "            # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
        "            tweet = ''.join(c for c in tweet if c <= '\\uFFFF')\n",
        "            #fix weirdly encoded texts\n",
        "            tweet = ftfy.fix_text(tweet)\n",
        "            #expand contraction\n",
        "            tweet = expandContractions(tweet)\n",
        "            #remove punctuation\n",
        "            tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n",
        "            \n",
        "            neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], tweet)\n",
        "            letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
        "            \n",
        "            # Tokenize and join to remove unneccessary white spaces\n",
        "            words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
        "            #return (\" \".join(words)).strip()\n",
        "\n",
        "            #stop words\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            stop_words.update((\"mon\",\"tue\",\"wed\",\"thu\",\"fri\",\"sat\",\"sun\",\"sunday\",\"monday\",\"tuesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"thurs\",\"thur\",\"tues\"))\n",
        "            stop_words.update((\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\n",
        "              \"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\n",
        "              \"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\", \"twitter\", \"thanking\",\"thanks\"))\n",
        "    \n",
        "            word_tokens = nltk.word_tokenize(tweet) \n",
        "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "            tweet = ' '.join(filtered_sentence)\n",
        "\n",
        "            #stemming words\n",
        "            tweet = PorterStemmer().stem(tweet)\n",
        "            \n",
        "            cleaned_tweets.append(tweet)\n",
        "\n",
        "    return cleaned_tweets\n",
        "\n",
        "def tokenize(cleaned_tweets):\n",
        "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(cleaned_tweets)\n",
        "    sequences_d = tokenizer.texts_to_sequences(cleaned_tweets) \n",
        "    wi = tokenizer.word_index\n",
        "    data_d = pad_sequences(sequences_d, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    return data_d\n",
        "\n",
        "def Predict(data_d):\n",
        "    output = model.predict(data_d)\n",
        "    # print(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "L58HBdP04Ngv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_sqlalchemy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE2MWz6E4fEQ",
        "outputId": "5bb189b3-264d-43c5-c85a-8ad8ebec0d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask_sqlalchemy in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: Flask>=2.2 in /usr/local/lib/python3.7/dist-packages (from flask_sqlalchemy) (2.2.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.18 in /usr/local/lib/python3.7/dist-packages (from flask_sqlalchemy) (1.4.41)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=2.2->flask_sqlalchemy) (3.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=2.2->flask_sqlalchemy) (4.12.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from Flask>=2.2->flask_sqlalchemy) (2.2.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=2.2->flask_sqlalchemy) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=2.2->flask_sqlalchemy) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->Flask>=2.2->flask_sqlalchemy) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->Flask>=2.2->flask_sqlalchemy) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->Flask>=2.2->flask_sqlalchemy) (2.1.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.4.18->flask_sqlalchemy) (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from flask import Flask, render_template, request\n",
        "# from flask_sqlalchemy import SQLAlchemy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1RhCMVw4PoD",
        "outputId": "321fba8e-56d7-4b31-f552-acfe767761a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihmgW_6b6NaQ",
        "outputId": "27e01fd7-71ac-4766-8039-301bfe3c8daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ftfy"
      ],
      "metadata": {
        "id": "tCFqUb4165g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = manuscipt.iloc[0:,0]\n",
        "# for tweet in manuscipt.data:\n",
        "#     data.append(tweet.text)\n",
        "\n",
        "cleaned_tweets = clean_tweets(data)\n",
        "tokenized_tweets = tokenize(cleaned_tweets)\n",
        "result = Predict(tokenized_tweets)  \n",
        "# print(result) \n",
        "sum = 0;     \n",
        "for r in result:\n",
        "    sum = sum + r[0];\n",
        "avg = sum/result.shape[0]\n",
        "avg "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lKm5Xv84rG3",
        "outputId": "742310b4-55b7-4c3c-ae59-43313e1fcf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9809357948686884"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NB_WORDS = 25000\n",
        "MAX_SEQUENCE_LENGTH = 280"
      ],
      "metadata": {
        "id": "EzX58vJX4bez"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}